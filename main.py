"""
main.py

å°ˆæ¡ˆåŸ·è¡Œå…¥å£ã€‚
æä¾›å…©ç¨®æ¨¡å¼ï¼š
1. æå–æ¨¡å¼ (extract-cookie)ï¼šæ‰‹å‹•ç™»å…¥ YouTube ä¸¦å­˜ä¸‹ state.jsonã€‚
2. çˆ¬å–æ¨¡å¼ (scrape)ï¼šè¼‰å…¥å·²å­˜æª”çš„ state.jsonï¼Œè‡ªå‹•ç²å–è§€çœ‹ç´€éŒ„ã€‚
"""
import os
import sys
import json
import sqlite3
import shutil
import asyncio
import argparse

# å¼·åˆ¶å°‡ Windows çµ‚ç«¯æ©Ÿè¼¸å‡ºè¨­ç‚º UTF-8ï¼Œé¿å… emoji å ±éŒ¯ (cp950)
if sys.stdout.encoding.lower() != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')

from dotenv import load_dotenv
from playwright.async_api import async_playwright
from src.scraper import handle_response, scroll_to_load_more, extract_history_items
from src.data_handler import save_to_json, load_existing_json

# è¼‰å…¥ .env è®Šæ•¸
load_dotenv()

async def _extract_firefox_cookie(session_dir: str):
    """æå– Firefox æœ¬æ©Ÿ Cookie"""
    # Firefox è¼ƒé›£ç¹éè‡ªå‹•åŒ–åµæ¸¬ï¼Œé€™è£¡æ”¹æ¡ç›´æ¥è®€å–æœ¬æ©Ÿ Firefox çš„ cookies.sqlite
    print("å•Ÿå‹• Firefox Cookie æå–æ¨¡å¼... æº–å‚™è®€å–æ‚¨çš„æœ¬æ©Ÿ Firefox ç™»å…¥ç‹€æ…‹ã€‚")
    print("âš ï¸ åŸ·è¡Œå‰ï¼Œè«‹ç¢ºä¿æ‚¨å·²ç¶“ã€Œå®Œå…¨é—œé–‰ã€äº† Firefox è¦–çª—ï¼")
    input("ğŸ‘‰ ç¢ºèªå·²é—œé–‰å¾Œï¼ŒæŒ‰ä¸‹ Enter éµç¹¼çºŒ...")
    
    appdata = os.getenv('APPDATA')
    if not appdata:
        print("âŒ æ‰¾ä¸åˆ° APPDATA ç’°å¢ƒè®Šæ•¸ã€‚")
        return
        
    firefox_profiles_dir = os.path.join(appdata, 'Mozilla', 'Firefox', 'Profiles')
    if not os.path.exists(firefox_profiles_dir):
        print(f"âŒ æ‰¾ä¸åˆ° Firefox ä½¿ç”¨è€…è³‡æ–™å¤¾ï¼š{firefox_profiles_dir}")
        return
        
    # å°‹æ‰¾æœ‰ youtube cookies çš„ç›®éŒ„
    target_db = None
    for p in os.listdir(firefox_profiles_dir):
        if p.endswith('.default-release') or p.endswith('.default'):
            db_path = os.path.join(firefox_profiles_dir, p, 'cookies.sqlite')
            if os.path.exists(db_path):
                target_db = db_path
                break
                
    if not target_db:
        print("âŒ æ‰¾ä¸åˆ° Firefox çš„ Cookie è³‡æ–™åº« (cookies.sqlite)ã€‚")
        return
        
    # ç‚ºäº†é¿å…é–å®šå•é¡Œï¼Œè¤‡è£½ä¸€ä»½æš«æ™‚çš„ db ä¾†è®€å–
    temp_db = "temp_cookies.sqlite"
    try:
        shutil.copy2(target_db, temp_db)
        conn = sqlite3.connect(temp_db)
        cur = conn.cursor()
        cur.execute("SELECT name, value, host, path, expiry, isSecure, isHttpOnly FROM moz_cookies WHERE host LIKE '%youtube.com%'")
        rows = cur.fetchall()
        
        if not rows:
            print("âŒ åœ¨æ‚¨çš„ Firefox ä¸­æ‰¾ä¸åˆ°ä»»ä½• YouTube çš„ Cookieã€‚è«‹å…ˆé€éä¸€èˆ¬çš„ Firefox é–‹å•Ÿ YouTube ä¸¦ç™»å…¥ä¸€æ¬¡ã€‚")
            return
            
        import time
        current_time = int(time.time())
        
        pw_cookies = []
        for r in rows:
            # ç¢ºä¿ expires æ˜¯ä¸€å€‹åˆç†çš„ UNIX timestamp (ç§’)ï¼Œéå¤§çš„å€¼ Playwright ä¸æ”¶
            expiry = r[4]
            if not expiry or expiry < 0 or expiry > current_time + 10 * 365 * 86400:
                expiry = -1
            else:
                expiry = int(expiry)
                
            pw_cookies.append({
                "name": r[0],
                "value": r[1],
                "domain": r[2],
                "path": r[3],
                "expires": expiry,
                "httpOnly": bool(r[6]),
                "secure": bool(r[5]),
                "sameSite": "Lax"
            })
            
        state_data = {
            "cookies": pw_cookies,
            "origins": []
        }
        
        # æ‰‹å‹•å‰µå»ºä¸€å€‹ profile ç›®éŒ„èˆ‡ Default/state.json ä¾†è®“å¾ŒçºŒçš„ scrape æ¨¡å¼èƒ½è®€åˆ°
        os.makedirs(os.path.join(session_dir, "Default"), exist_ok=True)
        with open(os.path.join(session_dir, "Default", "state.json"), "w", encoding="utf-8") as f:
            json.dump(state_data, f)
            
        print(f"âœ… æˆåŠŸå¾æœ¬æ©Ÿ Firefox æå– {len(rows)} ç­† YouTube Cookies è‡³ï¼š{session_dir}")
        
    except Exception as e:
        print(f"âŒ æå– Firefox Cookie æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    finally:
        if 'conn' in locals():
            conn.close()
        if os.path.exists(temp_db):
            os.remove(temp_db)

async def _extract_chrome_cookie(session_dir: str, headless: bool):
    """æå– Chrome Cookie"""
    print("å•Ÿå‹•ç™»å…¥æ¨¡å¼... è«‹åœ¨å½ˆå‡ºçš„ç€è¦½å™¨ä¸­æ‰‹å‹•ç™»å…¥ YouTubeã€‚")
    print("âš ï¸ é€™æ˜¯å°ˆå±¬æ–¼çˆ¬èŸ²çš„ç¨ç«‹ç€è¦½å™¨ï¼Œæ‚¨åªéœ€è¦åœ¨é€™è£¡ç™»å…¥ä¸€æ¬¡ã€‚")
    
    # ç¢ºä¿å„²å­˜ profile çš„è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(session_dir, exist_ok=True)
    
    async with async_playwright() as p:
        browser_launcher = p.chromium
        launch_kwargs = {
            "channel": "chrome",
            "args": [
                "--disable-blink-features=AutomationControlled",
                "--disable-infobars"
            ]
        }
            
        # ä½¿ç”¨ launch_persistent_context å•Ÿå‹•ï¼Œæ‰€æœ‰ç´€éŒ„ï¼ˆCookie, LocalStorage ç­‰ï¼‰æœƒè‡ªå‹•å­˜å…¥ session_dir
        context = await browser_launcher.launch_persistent_context(
            user_data_dir=session_dir,
            headless=headless,
            **launch_kwargs
        )
        
        page = await context.new_page()
        await page.goto("https://www.youtube.com")
        
        # é˜»å¡ç­‰å¾…ç”¨æˆ¶æ‰‹å‹•ç™»å…¥
        input("ğŸ‘‰ ç™»å…¥å®Œæˆå¾Œï¼ŒæŒ‰ä¸‹ Enter éµé—œé–‰ç€è¦½å™¨ä¸¦å„²å­˜ç‹€æ…‹...")
        
        await context.close()
        print(f"âœ… ç™»å…¥ç‹€æ…‹å·²æ°¸ä¹…ä¿å­˜è‡³ï¼š{session_dir}ï¼Œå¾ŒçºŒå¯ç›´æ¥ä½¿ç”¨è‡ªå‹•çˆ¬å–æ¨¡å¼ã€‚")

async def run_extract_cookie_mode(session_dir: str, headless: bool, browser_type: str = "chrome"):
    """
    æ–°å»ºä¸€å€‹ç¨ç«‹çš„ç€è¦½å™¨è¨­å®šæª”è®“ä½¿ç”¨è€…æ‰‹å‹•ç™»å…¥ YouTubeã€‚
    ä¸‹æ¬¡ç¨‹å¼åŸ·è¡Œæ™‚ï¼Œåªè¦è¼‰å…¥åŒä¸€å€‹è¨­å®šæª”ï¼Œå°±æœƒæŒçºŒä¿æŒç™»å…¥ç‹€æ…‹ã€‚
    """
    if browser_type == "firefox":
        await _extract_firefox_cookie(session_dir)
    else:
        await _extract_chrome_cookie(session_dir, headless)

async def run_scrape_mode(session_dir: str, headless: bool, browser_type: str = "chrome"):
    """
    è¼‰å…¥å°ˆå±¬çš„ç€è¦½å™¨è¨­å®šæª”ï¼Œä»¥ç™»å…¥ç‹€æ…‹å•Ÿå‹•çˆ¬èŸ²é‚è¼¯ã€‚
    """
    print("å•Ÿå‹•è‡ªå‹•çˆ¬å–æ¨¡å¼...")
    
    async with async_playwright() as p:
        if browser_type == "firefox":
            # Firefox ä½¿ç”¨å‰›å‰›æˆ‘å€‘è½‰æ›å‡ºçš„ state.json
            browser = await p.firefox.launch(headless=headless)
            state_path = os.path.join(session_dir, "Default", "state.json")
            if not os.path.exists(state_path):
                print("âŒ æ‰¾ä¸åˆ° Firefox çš„ç‹€æ…‹æª”ï¼Œè«‹å…ˆåŸ·è¡Œ `--mode extract-cookie --browser firefox`ã€‚")
                await browser.close()
                return
            context = await browser.new_context(storage_state=state_path)
        else:
            if not os.path.exists(session_dir) or not os.listdir(session_dir):
                print("âŒ æ‰¾ä¸åˆ°ç™»å…¥ç‹€æ…‹ï¼Œè«‹å…ˆåŸ·è¡Œ `--mode extract-cookie` é€²è¡Œæ‰‹å‹•ç™»å…¥ã€‚")
                return
                
            browser_launcher = p.chromium
            launch_kwargs = {
                "channel": "chrome",
                "args": [
                    "--disable-blink-features=AutomationControlled",
                    "--disable-infobars"
                ]
            }
            
            # ç›´æ¥æ›è¼‰å‰›å‰›å­˜å¥½ç™»å…¥ç‹€æ…‹çš„ profile è³‡æ–™å¤¾
            context = await browser_launcher.launch_persistent_context(
                user_data_dir=session_dir,
                headless=headless,
                **launch_kwargs
            )

            
        page = await context.new_page()
        
        # å»ºç«‹ä¸€å€‹å­—å…¸ä¾†é›†ä¸­ç®¡ç†æ””æˆªä¸‹ä¾†çš„å­—å¹•è³‡æ–™ (key: video_id, value: json subtitle)
        subtitle_store = {}
        
        # ç¶å®šç¶²è·¯æ””æˆªå™¨ï¼šç•¶ç¶²é èƒŒæ™¯ç™¼é€ responseï¼Œå°±æœƒè§¸ç™¼ handle_response
        page.on("response", lambda response: asyncio.ensure_future(
            handle_response(response, subtitle_store)
        ))
        
        print(f"ğŸ‘‰ æ­£åœ¨å‰å¾€ YouTube æ­·å²ç´€éŒ„é é¢...")
        await page.goto("https://www.youtube.com/feed/history")
        
        print("âœ… æˆåŠŸé€²å…¥æ­·å²ç´€éŒ„é é¢ï¼é–‹å§‹è¼‰å…¥èˆŠç´€éŒ„ä¸¦æŒçºŒæ””æˆªå­—å¹•...")
        
        # å‘ä¸‹æ²å‹• 5 æ¬¡ä»¥ç²å¾—æ›´å¤šå½±ç‰‡
        await scroll_to_load_more(page, max_scrolls=5)
        
        # è§£æ DOMï¼Œç²å–å½±ç‰‡æ¨™é¡Œè·Ÿç¶²å€
        history_videos = await extract_history_items(page)
        
        # æº–å‚™è¼¸å‡ºå„²å­˜è·¯å¾‘
        from urllib.parse import urlparse, parse_qs
        output_path = os.path.join(os.getcwd(), "data", "output", "history_dataset.json")
        
        # è¼‰å…¥ç¾å­˜çš„è³‡æ–™åº«ä»¥é˜²æ­¢é‡è¤‡æŠ“å–
        existing_data = load_existing_json(output_path)
        existing_video_ids = {item.get('video_id') for item in existing_data if item.get('video_id')}
        print(f"ğŸ“¦ ç›®å‰æœ¬åœ°è³‡æ–™åº«å·²å­˜æœ‰ {len(existing_video_ids)} ç­†æ­·å²ç´€éŒ„ã€‚")
        
        # å°‡æ””æˆªåˆ°çš„å­—å¹•è³‡æ–™ï¼Œå°æ‡‰åˆä½µåˆ°æ­·å²ç´€éŒ„ä¸­
        new_records = []
        for v in history_videos:
            video_url = v.get("url", "")
            parsed_url = urlparse(video_url)
            
            video_id = None
            # å¸¸è¦å½±ç‰‡: https://www.youtube.com/watch?v=VIDEO_ID
            if 'v' in parse_qs(parsed_url.query):
                video_id = parse_qs(parsed_url.query)['v'][0]
            # Shorts: https://www.youtube.com/shorts/VIDEO_ID
            elif '/shorts/' in parsed_url.path:
                video_id = parsed_url.path.split('/shorts/')[-1].split('/')[0]
                
            if not video_id:
                continue
                
            # ğŸ’¡ é˜²é‡è¤‡æ©Ÿåˆ¶ï¼šå› ç‚ºè§€çœ‹ç´€éŒ„æ˜¯å¾æ–°åˆ°èˆŠæ’åºçš„ï¼Œ
            # ç•¶æˆ‘å€‘é‡åˆ°æ—©å°±å­˜éçš„ video_idï¼Œå¯ä»¥é¸æ“‡è·³éæˆ–æ˜¯ç›´æ¥ç•¶ä½œçˆ¬å–ç•Œç·šã€‚
            # é€™è£¡æˆ‘å€‘æ¡å–å®‰å…¨çš„ã€Œé‡åˆ°é‡è¤‡çš„å°±è·³éä¸åŠ å…¥ã€ç­–ç•¥
            if video_id in existing_video_ids:
                continue
            
            # å¦‚æœæ””æˆªçš„æ¸…å–®è£¡å‰›å¥½æœ‰é€™éƒ¨å½±ç‰‡çš„å­—å¹•ï¼Œå°±å¡é€²å»
            subtitle_data = subtitle_store.get(video_id) if video_id else None
            
            new_records.append({
                "video_id": video_id,
                "title": v.get("title"),
                "url": video_url,
                "subtitle": subtitle_data
            })
            
            # åŠ å…¥å·²å­˜åœ¨çš„é›†åˆä¸­ï¼Œé˜²æ­¢æˆ‘å€‘è‡ªå·±é€™æ¬¡æ²å‹•æŠ“åˆ°é‡è¤‡çš„ DOM å…ƒç´ 
            existing_video_ids.add(video_id)
            
        if new_records:
            # æŠŠæ–°çš„ç´€éŒ„æ’åœ¨æœ€å‰é¢ (ä¿æŒæ™‚é–“å¾æ–°åˆ°èˆŠ)
            merged_results = new_records + existing_data
            save_to_json(merged_results, output_path)
            print(f"ğŸ‰ çˆ¬å–çµæŸï¼æˆåŠŸæ–°å¢ {len(new_records)} ç­†ç´€éŒ„ï¼Œå…¶ä¸­æ””æˆªåˆ° {len(subtitle_store)} ç­†å­—å¹•ã€‚ç›®å‰ç¸½æ”¶éŒ„ï¼š{len(merged_results)} ç­†ã€‚")
        else:
            print("âœ¨ çˆ¬å–çµæŸï¼æ²’æœ‰ç™¼ç¾æ–°çš„æ­·å²ç´€éŒ„ï¼Œç„¡é ˆæ›´æ–° JSONã€‚")
        await context.close()

def main():
    parser = argparse.ArgumentParser(description="YouTube History Scraper")
    parser.add_argument(
        "--mode", 
        type=str, 
        choices=["extract-cookie", "scrape"], 
        default="scrape",
        help="åŸ·è¡Œæ¨¡å¼ã€‚'extract-cookie' ç”¨æ–¼æ‰‹å‹•ä¿å­˜ç™»å…¥ç‹€æ…‹ï¼Œ'scrape' ç”¨æ–¼è‡ªå‹•æŠ“å–è³‡æ–™ã€‚"
    )
    
    parser.add_argument(
        "--browser",
        type=str,
        choices=["chrome", "firefox"],
        default="chrome",
        help="ä½¿ç”¨çš„ç€è¦½å™¨å¼•æ“ï¼Œå¯é¸ chrome æˆ– firefoxã€‚"
    )
    
    args = parser.parse_args()
    
    # ç”¨è³‡æ–™å¤¾ä¾†æ”¾ persistent profile
    session_dir = os.path.join(os.getcwd(), "data", "session", "playwright_profile")
    headless_env = os.getenv("HEADLESS", "False").lower() == "true"
    
    # å¦‚æœæ˜¯ç™»å…¥æ¨¡å¼ï¼Œå¼·åˆ¶é–‹å•Ÿç€è¦½å™¨ä»‹é¢
    headless = False if args.mode == "extract-cookie" else headless_env

    # æ ¹æ“šä¸åŒæ¨¡å¼å‘¼å«å°æ‡‰çš„éåŒæ­¥å‡½å¼
    if args.mode == "extract-cookie":
        asyncio.run(run_extract_cookie_mode(session_dir, headless, args.browser))
    else:
        asyncio.run(run_scrape_mode(session_dir, headless, args.browser))

if __name__ == "__main__":
    main()
